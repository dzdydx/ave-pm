# AVE-PM: An Audio-visual Event Localization Dataset for Portrait Mode Short Videos

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![License](https://img.shields.io/badge/license-MIT-green.svg)

**Update**: The AVE-PM project page is now available - [check it out!](https://dzdydx.github.io/ave-pm-homepage/)

**AVE-PM** is the first audio-visual event localization (AVEL) dataset specifically designed for **portrait-mode short videos**. It contains:

- ğŸ“¹ **25,335** video clips
- ğŸ”Š **86** fine-grained event categories
- â±ï¸ **Frame-level annotations** for precise temporal localization
- ğŸ¶ **Sample-level annotations** indicating background music presence

---

## ğŸ“¦ Installation

1. **Clone the repository**

```bash
git clone git@github.com:dzdydx/ave-pm.git
cd ave-pm-main

conda env create -f environment.yaml
conda activate AVE-PM
```

## ğŸ“ Dataset

You can download AVE-PM dataset from [Baidu Cloud Link](https://pan.baidu.com/s/1ErDp1zVEe0mugVMmQFbqow?pwd=2979). And then unzip the video files into the `dataset/data/videos/` folder.

### âš™ï¸ Data Preparation
Before training and testing, you need to preprocess the data by extracting audio and visual features.

ğŸ”‰ **Feature Extraction**

Initially, you can use the provided feature extraction script to extract features from the raw audio and visual features in AVEPM dataset. The script is located at `scripts_helper/encode.py`

``` bash 
bash scripts_helper/get_raw_feature.sh
```

This script will extract audio and visual features from the raw videos and store them in the `dataset/data/features` folder.

You can also use your own feature extraction method if desired.

ğŸ§  **Event Template and Preprocessing**
Secondly, you need to get the `event_templates.pkl` file for audio preprocessing. And then, you can process audio/video and extract audio/visual features in different methods using the following command:
``` bash
# Step 1: get the event_templates.pkl file
bash scripts_helper/get_template.sh

# Step 2: process audio/video and extract audio/visual features
bash scripts_helper/run_preprocess.sh
```
You may modify the parameters in the `.sh` files to suit your needs.

You may also use your own feature extraction method if desired.


ğŸï¸ **Extract Frames & Audio for LAVISH**

Before training the LAVISH model, you should extract video frames and raw audios, and putting them into `/dataset/data/video_frames` and `/dataset/data/raw_audios` folder.

```python
python /LAVISH/scripts/extract_frames.py --out_dir /dataset/data/video_frames/ --video_dir dataset/data/videos/
python /LAVISH/scripts/extract_audio.py --video_pth dataset/data/videos/ --save_pth dataset/data/raw_audios
```
The outputs will be saved to:

- Frames â†’ `/dataset/data/video_frames/`
- Audios â†’ `/dataset/data/raw_audios/`
  

### âš™ï¸ Cross-mode evaluation
To demonstrate the domain differences between landscape mode (LM) and portrait mode(PM) videos in the context of audio-visual event localization, we conducted a cross-mode evaluation on the S-LM and S-PM subsets. For a rigorous comparison, we selected 10 overlapping categories from AVE dataset and AVE-PM. Initially, we ultilize all samples from the corresponding categories of the AVE dataset to build the S-LM, which comprises 1536 samples, accounting for 37% of the total 4143 samples in AVE dataset. Subsequently, we select an equal number of samples per category from the AVE-PM dataset to build the S-PM.

Initially, you need to download the AVE dataset and put videos under `dataset/data/AVE/videos` folder. Then, you can use the following command to generate features for S-LM and S-PM subsets:

```bash 
bash scripts_helper/get_select_dataset.sh
```
This will generate:
- AVE Features â†’ `dataset/data/AVE/feature`
- S-LM Features â†’ `dataset/feature/select/ave`
- S-PM Features â†’ `dataset/feature/select/avepm`

Extract raw frames and raw audios from AVE dataset:
```python
python /LAVISH/scripts/extract_frames.py --out_dir dataset/data/AVE/video_frames --video_dir dataset/data/AVE/videos/
python /LAVISH/scripts/extract_audio.py --video_pth dataset/data/AVE/videos/ --save_pth dataset/data/AVE/raw_audios
```
The outputs will be saved to:

- AVE Frames â†’ `/dataset/data/AVE/video_frames/`
- AVE Audios â†’ `/dataset/data/AVE/raw_audios/`

### ğŸ“‚ Directory Layout

```graphql
AVE-PM/
â”œâ”€â”€ dataset/
|	â”œâ”€â”€ csvfiles/			   # csv files for training,validating and testing
|	|   â”œâ”€â”€ select/			  # csv files for S-LM and S-PM subsets
|	|   |  â”œâ”€â”€ ave/
|   |   |  â””â”€â”€ avepm/
|   |	â”œâ”€â”€ AVE/
|   |	|	â”œâ”€â”€ data/          # AVE dataset csv files
|   |	|	â”œâ”€â”€ feature/
|   |	|	â”œâ”€â”€ raw_audios/
|   |	|	â”œâ”€â”€ video_frames/
|   |	|	â””â”€â”€ videos/        # AVE dataset videos
â”‚   â”‚   â”œâ”€â”€ videos/            # Raw portrait-mode videos
â”‚   â”‚   â”œâ”€â”€ video_frames/      # Extracted RGB frames (generated)
|	|	â”œâ”€â”€ processed_audios/  # Preprocessed audio segments(generated)
â”‚   â”‚   â””â”€â”€ raw_audios/        # Extracted audio segments (generated)
â”‚   â””â”€â”€ feature/			   # Precomputed audio and visual features
|       â”œâ”€â”€ features/  
|       â”œâ”€â”€ select/       # Precomputed features for S-LM and S-PM subsets
â”‚       â”œâ”€â”€ preprocess_audio_feature/   
|       â””â”€â”€ preprocess_visual_feature/
```



## ğŸš€ Training & Evaluation

We provide training and inference scripts for four baseline models evaluated on AVE-PM. You can use the following command to train or evaluate the models:

```bash
bash run.sh # Customize model and mode inside the script
```
Edit run.sh to set the model name and training/evaluation options.

**Note**: The `run.sh` script serves as a wrapper to configure the baseline model and specify whether to train or evaluate. You can customize its behavior by modifying the corresponding parameters in the corresponding scripts.

### âœ… Pretrained Checkpoints

You can download pretrained model checkpoints here:
 | Model | Checkpoint | 
 | --- | --- | 
 | AVEL | *(Coming Soon)* | 
 | CPSP | *(Coming Soon)* | 
 | CMBS | *(Coming Soon)* | 
 | LAVISH | *(Coming Soon)* | 



## ğŸ“Œ Citation

If you find this project helpful for your research, please consider citing:

```bibtex
@misc{liu2025audiovisualeventlocalizationportrait,
      title={Audio-visual Event Localization on Portrait Mode Short Videos}, 
      author={Wuyang Liu and Yi Chai and Yongpeng Yan and Yanzhen Ren},
      year={2025},
      eprint={2504.06884},
      archivePrefix={arXiv},
      primaryClass={cs.MM},
      url={https://arxiv.org/abs/2504.06884}, 
}
```



## ğŸ™ Acknowledgements

We thank the authors of the following methods and datasets for their valuable contributions:

- **AVEL (ECCV'18):**
   https://github.com/YapengTian/AVE-ECCV18
- **CPSP (CPSP@CVPR'22):**
   https://github.com/jasongief/cpsp
- **CMBS (CMBS@ICCV'23):**
   https://github.com/marmot-xy/CMBS
- **LAVISH (LAVISH@ICCV'23):**
   https://genjib.github.io/project_page/LAVISH/
- **ByteDance Portrait-Mode Video Recognition:**
   https://github.com/bytedance/portrait-video-recognition



