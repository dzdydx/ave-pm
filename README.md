# AVE-PM: An Audio-visual Event Localization Dataset for Portrait Mode Short Videos

![Python](https://img.shields.io/badge/Python-3.8+-blue.svg)
![License](https://img.shields.io/badge/license-CC%20BY--NC--SA%204.0-green.svg)

**Update**: The AVE-PM project page is now available - [check it out!](https://dzdydx.github.io/ave-pm-homepage/)

**AVE-PM** is the first audio-visual event localization (AVEL) dataset specifically designed for **portrait-mode short videos**. It contains:

- ğŸ“¹ **25,335** video clips
- ğŸ”Š **86** fine-grained event categories
- â±ï¸ **Frame-level annotations** for precise temporal localization
- ğŸ¶ **Sample-level annotations** indicating background music presence

---

## ğŸ“¦ Installation

1. **Clone the repository**

```bash
git clone git@github.com:dzdydx/ave-pm.git
cd ave-pm-main

conda env create -f environment.yaml
conda activate AVE-PM
```

## ğŸ“ Dataset

You can download AVE-PM dataset from [Baidu Cloud Link](https://pan.baidu.com/s/1ErDp1zVEe0mugVMmQFbqow?pwd=2979). And then unzip the video files into the `dataset/data/videos/` folder.

### âš™ï¸ Data Preparation
Prior to training and evaluation, you'll need to preprocess the data by extracting audio and visual features.

ğŸ”‰ **Feature Extraction**

Use the provided script to extract features from raw audio and video data. The script is located at `scripts_helper/encode.py`:

``` bash 
bash scripts_helper/get_raw_feature.sh
```

This will extract audio and visual features from the videos and store them in `dataset/data/features`.

ğŸ§  **Event Template and Preprocessing**
First generate the `event_templates.pkl` file for audio preprocessing, then process the audio/video data:

``` bash
# Step 1: get the event_templates.pkl file
bash scripts_helper/get_template.sh

# Step 2: process audio/video and extract audio/visual features
bash scripts_helper/run_preprocess.sh
```

Customize the parameters in the `.sh` files as needed. Alternative feature extraction methods may be used.


ğŸï¸ **Video Frames & Audio Extraction for LAVISH**

For LAVISH model training, extract video frames and raw audio into respective directories:

```python
python /LAVISH/scripts/extract_frames.py --out_dir /dataset/data/video_frames/ --video_dir dataset/data/videos/
python /LAVISH/scripts/extract_audio.py --video_pth dataset/data/videos/ --save_pth dataset/data/raw_audios
```

Output locations:
- Frames â†’ `/dataset/data/video_frames/`
- Audios â†’ `/dataset/data/raw_audios/`
  

### âš™ï¸ Cross-mode evaluation
To investigate domain differences between landscape (LM) and portrait mode (PM) videos in audio-visual event localization, we conducted cross-mode evaluations using the S-LM and S-PM subsets. We selected 10 overlapping categories from both AVE and AVE-PM datasets, creating:

1. S-LM: All samples from AVE's corresponding categories (1,536 samples, 37% of total AVE dataset)
2. S-PM: Balanced samples from AVE-PM's matching categories

First download the AVE dataset to `dataset/data/AVE/videos`, then generate features:

```bash 
bash scripts_helper/get_select_dataset.sh
```

This creates:
- AVE Features â†’ `dataset/data/AVE/feature`
- S-LM Features â†’ `dataset/feature/select/ave`
- S-PM Features â†’ `dataset/feature/select/avepm`

Extract AVE dataset frames and audio:

```python
python /LAVISH/scripts/extract_frames.py --out_dir dataset/data/AVE/video_frames --video_dir dataset/data/AVE/videos/
python /LAVISH/scripts/extract_audio.py --video_pth dataset/data/AVE/videos/ --save_pth dataset/data/AVE/raw_audios
```

Output locations:
- AVE Frames â†’ `/dataset/data/AVE/video_frames/`
- AVE Audios â†’ `/dataset/data/AVE/raw_audios/`

### ğŸ“‚ Directory Structure

```graphql
AVE-PM/
â”œâ”€â”€ dataset/
|	â”œâ”€â”€ csvfiles/			   # csv files for training,validating and testing
|	|   â”œâ”€â”€ select/			  # csv files for S-LM and S-PM subsets
|	|   |  â”œâ”€â”€ ave/
|   |   |  â””â”€â”€ avepm/
|   |	â”œâ”€â”€ AVE/
|   |	|	â”œâ”€â”€ data/          # AVE dataset csv files
|   |	|	â”œâ”€â”€ feature/
|   |	|	â”œâ”€â”€ raw_audios/
|   |	|	â”œâ”€â”€ video_frames/
|   |	|	â””â”€â”€ videos/        # AVE dataset videos
â”‚   â”‚   â”œâ”€â”€ videos/            # Raw portrait-mode videos
â”‚   â”‚   â”œâ”€â”€ video_frames/      # Extracted RGB frames (generated)
|	|	â”œâ”€â”€ processed_audios/  # Preprocessed audio segments(generated)
â”‚   â”‚   â””â”€â”€ raw_audios/        # Extracted audio segments (generated)
â”‚   â””â”€â”€ feature/			   # Precomputed audio and visual features
|       â”œâ”€â”€ features/  
|       â”œâ”€â”€ select/       # Precomputed features for S-LM and S-PM subsets
â”‚       â”œâ”€â”€ preprocess_audio_feature/   
|       â””â”€â”€ preprocess_visual_feature/
```

## ğŸš€ Training & Evaluation

We provide scripts for four baseline models. Run evaluations using:

```bash
bash run.sh # Customize model and mode inside the script
```

Modify `run.sh` to select models and specify training/evaluation modes.

**Note**: The `run.sh` script serves as a configuration wrapper. Adjust parameters in the corresponding scripts as needed.

### âœ… Pretrained Checkpoints

Available checkpoints:
 | Model | Checkpoint | 
 | --- | --- | 
 | AVEL | *(Coming Soon)* | 
 | CPSP | *(Coming Soon)* | 
 | CMBS | *(Coming Soon)* | 
 | LAVISH | *(Coming Soon)* | 



## ğŸ“Œ Citation

If you find this project helpful, please consider citing:

```bibtex
@misc{liu2025audiovisualeventlocalizationportrait,
      title={Audio-visual Event Localization on Portrait Mode Short Videos}, 
      author={Wuyang Liu and Yi Chai and Yongpeng Yan and Yanzhen Ren},
      year={2025},
      eprint={2504.06884},
      archivePrefix={arXiv},
      primaryClass={cs.MM},
      url={https://arxiv.org/abs/2504.06884}, 
}
```



## ğŸ™ Acknowledgements

We acknowledge the following foundational works:

- **AVEL (ECCV'18):**
   https://github.com/YapengTian/AVE-ECCV18
- **CPSP (CPSP@CVPR'22):**
   https://github.com/jasongief/cpsp
- **CMBS (CMBS@ICCV'23):**
   https://github.com/marmot-xy/CMBS
- **LAVISH (LAVISH@ICCV'23):**
   https://genjib.github.io/project_page/LAVISH/
- **ByteDance Portrait-Mode Video Recognition:**
   https://github.com/bytedance/Portrait-Mode-Video


## ğŸ“„ License

This project is licensed under the [Creative Commons Attribution-NonCommercial-ShareAlike 4.0 International License](LICENSE).
